# -*- coding: utf-8 -*-
"""PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRsz2_V3IFbvBuWms2JLKnAZ81VmwRIt
"""

#!pip install --upgrade scikit-learn -q --user
# need to restart kernel, if latest versions not already installed

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'retina'
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', context='talk', font_scale=0.8, rc={'figure.figsize': (12,6)} )
from scipy import stats
import datetime
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder,OrdinalEncoder

import numpy as np
import pandas as pd
import seaborn as sns
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from __future__ import print_function  # Required for Python 2.x
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from google.colab import drive
drive.mount('/content/drive')

news_data=pd.read_csv("/content/drive/MyDrive/all_data(1).csv")

news_data.shape

news_data.head()

news_data.columns

news_data['type'].value_counts

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

"""###CLASS DISTRIBUTION"""

#class_distribution = news_data['type'].value_counts()
#print(class_distribution)

import matplotlib.pyplot as plt

#class_distribution = news_data['type'].value_counts()

#colors = ['red', (0, 0, 0.5)]  # Define colors for each class, using RGB values for deep blue
#labels = ['Fake News', 'Real News']  # Define labels for each class

#plt.bar(class_distribution.index, class_distribution.values, color=colors)
#plt.xlabel('Type')
#plt.ylabel('Count')
#plt.title('News Class Distribution')

# Add labels to the bars
#for i, count in enumerate(class_distribution.values):
    #plt.text(i, count, str(count), ha='center', va='bottom')

# Add legend
#plt.legend(labels, loc='upper right')

#plt.show()

"""###MISSING DATA ANALYSIS"""

# counting the number of missing values in the dataset
#news_data.isnull().sum()

# to check missing values
#sns.heatmap(news_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')

"""###DROP AN UNWANTED COLUMN"""

# Drop unwanted columns
news_data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'ord_in_thread', 'replies_count', 'participants_count', 'country','likes', 'shares', 'comments', 'site_url', 'language', 'ord_in_thread', 'uuid', 'crawled', 'spam_score','id','published'], axis=1, inplace=True)

# counting the number of missing values in the dataset
news_data.isnull().sum()

import plotly.graph_objects as go
import pandas as pd

# Assuming `news_data` is your dataset containing the 'type' column

# Define colors for each class
colors = ['red', 'rgb(0, 0, 128)']  # 'red' for 'FAKE' and deep blue for 'REAL'

labels = ['FAKE', 'REAL']
values = news_data['type'].value_counts() / news_data['type'].shape[0]

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_traces(hovertemplate='%{label}: %{percent}', textinfo='percent', textfont_size=20,
                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))  # Uncomment this line
fig.update_layout(
    title_text="Target Balance",
    title_font_color="white",
    legend_title_font_color="yellow",
    paper_bgcolor="black",
    plot_bgcolor='black',
    font_color="white",
    legend=dict(x=0, y=0),  # Adjust the legend position
)
fig.show()

#To show heatmap data should be in matrix form.
#tc = news_data.corr()   #shows corelation in matrix form
#tc

from PIL import Image
import requests
import numpy as np
from PIL import Image
from io import BytesIO
from wordcloud import WordCloud
import matplotlib.pyplot as plt

!pip install wordcloud
import requests
from wordcloud import WordCloud, STOPWORDS  #install $ pip install wordcloud


def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0),
                   title = None, title_size=40, image_color=False):
    stopwords = set(STOPWORDS)
    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}
    stopwords = stopwords.union(more_stopwords)

    wordcloud = WordCloud(background_color='white',
                    stopwords = stopwords,
                    max_words = max_words,
                    max_font_size = max_font_size,
                    random_state = 42,
                    width=800,
                    height=400,
                    mask = mask)
    wordcloud.generate(str(text))

    plt.figure(figsize=figure_size)
    if image_color:
        image_colors = ImageColorGenerator(mask);
        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation="bilinear");
        plt.title(title, fontdict={'size': title_size,
                                  'verticalalignment': 'bottom'})
    else:
        plt.imshow(wordcloud);
        plt.title(title, fontdict={'size': title_size, 'color': 'green',
                                  'verticalalignment': 'bottom'})
    plt.axis('off');
    plt.tight_layout()

#Word cloud for real words
response = requests.get('https://raw.githubusercontent.com/manojknit/Natural_Language_Processing/master/images/upvote.png')
upvote_mask = np.array(Image.open(BytesIO(response.content)))#https://raw.githubusercontent.com/manojknit/Natural_Language_Processing/master/images/upvote.png
plot_wordcloud(news_data[news_data["type"] == "real"]["text"], upvote_mask, max_words=300000, max_font_size=300, title="Word Cloud of Real News")

#Word cloud for Fake words
response = requests.get('https://image.freepik.com/free-icon/thumbs-down-silhouette_318-41911.jpg')
upvote_mask = np.array(Image.open(BytesIO(response.content)))#https://raw.githubusercontent.com/manojknit/Natural_Language_Processing/master/images/upvote.png
plot_wordcloud(news_data[news_data["type"] == "fake"]["text"], upvote_mask, max_words=300000, max_font_size=300, title="Word Cloud of Fake News")

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Combine the 'text' column into a single string
combined_text = ' '.join(news_data['text'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)

# Display the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Text Column")
plt.show()

"""###TEXT LENGTH"""

import matplotlib.pyplot as plt

# Group the data by 'type' and 'text_len'
#grouped_data = news_data.groupby('type')['text_len']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Specify colors for each box
#box_colors = {'real': 'blue', 'fake': 'red'}  # Make sure labels match the case in your data

# Increase the figure size
#plt.figure(figsize=(8, 4))

# Plot the boxplot with specified colors
#boxplot = plt.boxplot(data_by_type, labels=grouped_data.groups, patch_artist=True)

# Set facecolors for each box
#for box, color in zip(boxplot['boxes'], [box_colors[group.lower()] for group in grouped_data.groups]):
    #box.set_facecolor(color)

# Set plot title and labels
#plt.title('Text Length by Type')
#plt.xlabel('Type')
#plt.ylabel('Text Length')

# Display the plot
#plt.show()

"""###EXCLAMATION LENGTH"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))

#grouped_data = news_data.groupby('type')['excl_text']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
#plt.boxplot(data_by_type, labels=grouped_data.groups)
#plt.title('Exclamation Length by Type')
#plt.xlabel('Type')
#plt.ylabel('Exclamation Length')

# Display the plot
#plt.show()

"""###EXCLAMATION TITLE"""

import pandas as pd
import matplotlib.pyplot as plt
#plt.figure(figsize=(8, 4))  # Adjust the figure size as needed
#grouped_data = news_data.groupby('type')['excl_title']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
#plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
#plt.title('Exclamation Title by Type')
#plt.xlabel('Type')
#plt.ylabel('Exclamation title')
# Display the plot
#plt.show()

"""###FIRST TITLE"""

import pandas as pd
import matplotlib.pyplot as plt

# Group the data by 'type' and 'first_title'
#grouped_data = news_data.groupby('type')['first_title']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Specify colors for each box
#box_colors = {'real': 'blue', 'fake': 'red'}  # Make sure labels match the case in your data

# Increase the figure size
#plt.figure(figsize=(8, 4))

# Plot the boxplot with specified colors
#boxplot = plt.boxplot(data_by_type, labels=grouped_data.groups, patch_artist=True)

# Set facecolors for each box
#for box, color in zip(boxplot['boxes'], [box_colors[group.lower()] for group in grouped_data.groups]):
    #box.set_facecolor(color)

# Set plot title and labels
#plt.title('First Title by Type')
#plt.xlabel('Type')
#plt.ylabel('First Title')

# Rotate x-axis labels for better readability (optional)
#plt.xticks(rotation=45)

# Display the plot
#plt.show()

"""###SECOND TITLE"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
#grouped_data = news_data.groupby('type')['second_title']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
#plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
#plt.title('Second Title by Type')
#plt.xlabel('Type')
#plt.ylabel('Second Title')

# Rotate x-axis labels for better readability (optional)
#plt.xticks(rotation=45)

# Display the plot
#plt.show()

"""###SECOND TEXT"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed
grouped_data = news_data.groupby('type')['second_text']

# Create a list to store the data for each class
#data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
#plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
#plt.title('Second Text by Type')
#plt.xlabel('Type')
#plt.ylabel('Second Text')

# Rotate x-axis labels for better readability (optional)
#plt.xticks(rotation=45)

# Display the plot
#plt.show()

"""###TITLE LENGTH"""

import pandas as pd
import matplotlib.pyplot as plt

# Group the data by 'type' column and extract the 'title_len' column
grouped_data = news_data.groupby('type')['title_len']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Adjust the figure size as needed
plt.figure(figsize=(8, 4))

# Define the box colors
box_colors = {'real': 'blue', 'fake': 'red'}

# Plot the boxplot with customized box colors
boxplot = plt.boxplot(data_by_type, labels=grouped_data.groups, patch_artist=True)

# Set facecolors for each box
for box, color in zip(boxplot['boxes'], [box_colors[group.lower()] for group in grouped_data.groups]):
    box.set(facecolor=color)

# Set plot title and labels
plt.title('Title Length by Type')
plt.xlabel('Type')
plt.ylabel('Title Length')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###FIRST ALL"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed

# Group the data by 'type' column and extract the 'first title' column
grouped_data = news_data.groupby('type')['first_all']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
plt.title('First All by Type')
plt.xlabel('Type')
plt.ylabel('First All')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###SECOND_ALL"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed

# Group the data by 'type' column and extract the 'first title' column
grouped_data = news_data.groupby('type')['second_all']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
plt.title('Second All by Type')
plt.xlabel('Type')
plt.ylabel('Second All')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###THIRD ALL"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed

# Group the data by 'type' column and extract the 'first title' column
grouped_data = news_data.groupby('type')['third_all']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
plt.title('Third All by Type')
plt.xlabel('Type')
plt.ylabel('Third All')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###THIRD TITLE"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed

# Group the data by 'type' column and extract the 'first title' column
grouped_data = news_data.groupby('type')['third_title']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
plt.title('Third Title by Type')
plt.xlabel('Type')
plt.ylabel('Third Title')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###THIRD TEXT"""

import pandas as pd
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))  # Adjust the figure size as needed

# Group the data by 'type' column and extract the 'first title' column
grouped_data = news_data.groupby('type')['third_text']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Plot the boxplot
plt.boxplot(data_by_type, labels=grouped_data.groups)

# Set plot title and labels
plt.title('Third Text by Type')
plt.xlabel('Type')
plt.ylabel('Third Text')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""###The Negations in News"""

import pandas as pd
import matplotlib.pyplot as plt

# Group the data by 'type' column and extract the 'negative' column
grouped_data = news_data.groupby('type')['negative']

# Create a list to store the data for each class
data_by_type = [grouped_data.get_group(type_class) for type_class in grouped_data.groups]

# Adjust the figure size as needed
plt.figure(figsize=(8, 4))

# Define the box colors
box_colors = {'real': 'blue', 'fake': 'red'}

# Plot the boxplot with customized box colors
boxplot = plt.boxplot(data_by_type, labels=grouped_data.groups, patch_artist=True)

# Set facecolors for each box
for box, color in zip(boxplot['boxes'], [box_colors[group.lower()] for group in grouped_data.groups]):
    box.set(facecolor=color)

# Set plot title and labels
plt.title('Negations in News by Type')
plt.xlabel('Type')
plt.ylabel('Negative')

# Display the plot
plt.show()

plt.figure(figsize=(10, 6))  # Adjust the figure size as needed

# Group the data by 'type' column and calculate the median of 'positive' and 'negative' columns
median_sentiment = news_data.groupby('type')[['positive', 'negative']].median()

# Plot the median sentiment values
median_sentiment.plot(kind='bar')

# Set plot title and labels
plt.title('Median Sentiment Value by Type')
plt.xlabel('Type')
plt.ylabel('Median Sentiment')

# Display the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have the data in a DataFrame named 'df'
# df['type'] contains the classes, df['positive'] contains the positive sentiment values, and df['negative'] contains the negative sentiment values

plt.figure(figsize=(10, 6))  # Adjust the figure size as needed

# Create a new column 'class' based on the 'type' column
news_data['class'] = news_data['type'].apply(lambda x: 'Real' if x == 'real' else 'Fake')

# Group the data by 'class' column and calculate the median of 'positive' and 'negative' columns
median_sentiment = news_data.groupby('class')[['positive', 'negative']].median()

# Plot the median sentiment values
median_sentiment.plot(kind='bar')

# Set plot title and labels
plt.title('Median Sentiment Value by Class')
plt.xlabel('Class')
plt.ylabel('Median Sentiment')

# Display the plot
plt.show()

"""### Word Frequency plot of real and fake news:"""

from plotly import tools

from collections import defaultdict
news_data_real = news_data[news_data["type"]=='news']
news_data_fake = news_data[news_data["type"]!='news']

## custom function for ngram generation ##
def generate_ngrams(text, n_gram=1):
    token = [token for token in text.lower().split(" ") if token != "" if token not in STOPWORDS]
    ngrams = zip(*[token[i:] for i in range(n_gram)])
    return [" ".join(ngram) for ngram in ngrams]

## custom function for horizontal bar chart ##
def horizontal_bar_chart(news_data, color):
    trace = go.Bar(
        y=df["word"].values[::-1],
        x=df["wordcount"].values[::-1],
        showlegend=False,
        orientation = 'h',
        marker=dict(
            color=color,
        ),
    )
    return trace

def generate_ngrams(text, n):
    # Implementation of n-gram generation
    # Return an empty list if text is None or empty
    if text is None or text == "":
        return []
    # Your existing implementation
    pass

import plotly.express as px

fig = px.sunburst(news_data, path=['type'])
fig.show()

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]


def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

"""###TOP 20 UNIGRAM USED IN THE ARTICLE"""

import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer  # Add this import

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

common_words = get_top_n_words(news_data['text'], 20)
news_data2 = pd.DataFrame(common_words, columns=['word', 'count'])
sorted_data = news_data2.groupby('word').sum()['count'].sort_values(ascending=False)

fig = go.Figure(data=[go.Bar(x=sorted_data.index, y=sorted_data.values)])
fig.update_layout(
    title='Top 20 unigrams used in articles',
    xaxis_title='Word',
    yaxis_title='Count'
)
fig.show()

"""###TOP 20 BI-GRAM"""

import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

def get_top_n_bigrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

common_bigrams = get_top_n_bigrams(news_data['text'], 20)
news_data2 = pd.DataFrame(common_bigrams, columns=['word', 'count'])
sorted_data = news_data2.groupby('word').sum()['count'].sort_values(ascending=False)

fig = go.Figure(data=[go.Bar(x=sorted_data.index, y=sorted_data.values)])
fig.update_layout(
    title='Top 20 bigrams used in articles',
    xaxis_title='Bigram',
    yaxis_title='Count'
)
fig.show()

common_bigrams = get_top_n_bigrams(news_data['text'], 20)
news_data2 = pd.DataFrame(common_bigrams, columns=['word', 'count'])
sorted_data = news_data2.groupby('word').sum()['count'].sort_values(ascending=False)

fig = go.Figure(data=[go.Bar(x=sorted_data.index, y=sorted_data.values)])
fig.update_layout(
    title='Top 20 bigrams used in articles',
    xaxis_title='Bigram',
    yaxis_title='Count'
)
fig.show()

"""###TOP 20 TRIGRAMS USED IN THE ARTICLES"""

from nltk import ngrams
from collections import Counter

# Assuming you have a DataFrame named 'news_data' with 'text' and 'type' columns

# Filter the DataFrame for each class
fake_data = news_data[news_data['type'] == 'fake']
real_data = news_data[news_data['type'] == 'real']

# Generate trigrams for the 'fake' class
fake_trigrams = []
for text in fake_data['text']:
    fake_trigrams.extend(ngrams(text.split(), 3))  # Adjust the value of 'n' as needed
fake_trigram_counts = Counter(fake_trigrams)

# Generate trigrams for the 'real' class
real_trigrams = []
for text in real_data['text']:
    real_trigrams.extend(ngrams(text.split(), 3))  # Adjust the value of 'n' as needed
real_trigram_counts = Counter(real_trigrams)

# Print the most common trigrams for each class
print("Fake Trigrams:")
print(fake_trigram_counts.most_common(20))
print()
print("Real Trigrams:")
print(real_trigram_counts.most_common(20))

import plotly.graph_objects as go

# Assuming you have a DataFrame named 'news_data' with 'text' and 'type' columns

# Filter the DataFrame for each class
fake_data = news_data[news_data['type'] == 'fake']
real_data = news_data[news_data['type'] == 'real']

# Generate trigrams for the 'fake' class
fake_trigrams = []
for text in fake_data['text']:
    fake_trigrams.extend(ngrams(text.split(), 3))  # Adjust the value of 'n' as needed
fake_trigram_counts = Counter(fake_trigrams)

# Generate trigrams for the 'real' class
real_trigrams = []
for text in real_data['text']:
    real_trigrams.extend(ngrams(text.split(), 3))  # Adjust the value of 'n' as needed
real_trigram_counts = Counter(real_trigrams)

# Get the most common trigrams for each class
top_fake_trigrams = fake_trigram_counts.most_common(20)
top_real_trigrams = real_trigram_counts.most_common(20)

# Extract trigrams and counts for plotting
fake_trigram_labels, fake_trigram_values = zip(*top_fake_trigrams)
real_trigram_labels, real_trigram_values = zip(*top_real_trigrams)

# Create bar plots
fig = go.Figure()
fig.add_trace(go.Bar(x=fake_trigram_labels, y=fake_trigram_values, name='Fake Trigrams'))
fig.add_trace(go.Bar(x=real_trigram_labels, y=real_trigram_values, name='Real Trigrams'))

# Update layout
fig.update_layout(
    title='Most Common Trigrams',
    xaxis_title='Trigram',
    yaxis_title='Count',
    barmode='group'
)

# Show the plot
fig.show()

"""###WORDCLOUD OF ARTICLES"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc = WordCloud(background_color="black", max_words=100, max_font_size=256, random_state=42, width=1000, height=1000)
wc.generate(' '.join(news_data['text']))  # Replace 'text' with the correct column name
plt.imshow(wc, interpolation="bilinear")
plt.axis('off')
plt.show()

"""##T-SNE WITH REDUCED NUMBER OF WORDS IN THE VOCABULARY...."""

import nltk
import re

from gensim.models import word2vec
from sklearn.manifold import TSNE

def build_corpus(data):
    "Creates a list of lists containing words from each sentence"
    corpus = []
    for sentence in news_data["text"].iteritems():
        word_list = sentence[1].split(" ")
        corpus.append(word_list)

    return corpus

corpus = build_corpus(news_data)
#corpus[0:2] # can see individual tokens

def tsne_plot(model):
    "Creates and TSNE model and plots it"
    labels = []
    tokens = []

    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)

    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])

    plt.figure(figsize=(16, 16))
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

import pandas as pd
import re
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def build_corpus(data):
    corpus = []
    for sentence in data['text']:
        # Remove special characters and convert to lowercase
        cleaned_sentence = re.sub(r'[^a-zA-Z0-9\s]', '', sentence.lower())
        # Split the sentence into words
        words = cleaned_sentence.split()
        corpus.append(words)
    return corpus

# Assuming 'news_dataset' is the DataFrame you provided
corpus = build_corpus(news_data)

# Train Word2Vec model
model = Word2Vec(corpus, vector_size=100, window=20, min_count=500, workers=4)

# Reduce dimensionality of word vectors using t-SNE
tsne = TSNE(n_components=2, random_state=42)
word_vectors_tsne = tsne.fit_transform(model.wv.vectors)

# Select a subset of words for visualization
num_words = 100  # Adjust this number as desired
words = model.wv.index_to_key[:num_words]
word_vectors_tsne_subset = word_vectors_tsne[:num_words, :]

# Visualize word embeddings using a scatter plot
plt.figure(figsize=(10, 8))
plt.scatter(word_vectors_tsne_subset[:, 0], word_vectors_tsne_subset[:, 1], marker='.')
for i, word in enumerate(words):
    plt.annotate(word, xy=(word_vectors_tsne_subset[i, 0], word_vectors_tsne_subset[i, 1]), fontsize=8)
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.title('Word2Vec Word Embeddings')
plt.show()

"""###We can see tokens which are closer are similar..."""

similar_words = model.wv.most_similar('trump')
for word, similarity in similar_words:
    print(word, similarity)

import matplotlib.pyplot as plt

similar_words = model.wv.most_similar('trump')
words = [word for word, _ in similar_words]
similarities = [similarity for _, similarity in similar_words]

plt.bar(words, similarities)
plt.xlabel('Similar Words')
plt.ylabel('Similarity Score')
plt.title('Similar Words to "trump"')
plt.xticks(rotation=90)
plt.show()

# Create a new column 'label' based on the values in the 'type' column
news_data['label'] = news_data['type'].map({'fake': 1, 'real': 0})

# Display the updated DataFrame with the new 'label' column
print(news_data)

news_data.info()

news_data.head()

news_data['label'].value_counts()

# merging the author name and news title
news_data['content'] = news_data['author']+' '+news_data['title']

print(news_data['content'])

(news_data['content']).head()

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

"""###Stemming:

Stemming is the process of reducing a word to its Root word

example:
actor, actress, acting --> act
"""

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_data['content'] = news_data['content'].apply(stemming)

#separating the data and label
X = news_data['content'].values
y = news_data['label'].values

print(X)

print(y)

y.shape

# converting the textual data to numerical data
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

"""Splitting the dataset to training & test data"""

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=Y, random_state=2)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 100)

from sklearn.metrics import roc_curve, roc_auc_score, auc
# Function to get roc curve
def get_roc (y_test,y_pred):
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    #Plot of a ROC curve
    plt.figure()
    lw = 2
    plt.plot(fpr, tpr, color='darkorange',
             label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="upper left")
    plt.show()
    plt.figure(figsize=(8, 6))
    return

from sklearn.metrics import  average_precision_score, precision_recall_curve


# Function to get Precision recall curve
def get_prec_recall (y_test,y_pred):
    average_precision = average_precision_score(y_test, y_pred)
    print('Average precision-recall score : {}'.format(average_precision))
    precision, recall, _ = precision_recall_curve(y_test, y_pred)
    plt.step(recall, precision, color='b', alpha=0.2, where='post')
    plt.fill_between(recall, precision, step='post', alpha=0.2,color='cyan')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))
    plt.figure(figsize=(8, 6))
    return

"""##LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)

predictions = logmodel.predict(X_test)

#Getting feature importances
print(logmodel.coef_)

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

# Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = logmodel, X = X_train, y = y_train, cv = 10)
accuracies.mean()

from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

cnf_matrix_logreg = confusion_matrix(y_test, predictions)

# Set the size of the figure
plt.figure(figsize=(8, 6))

# Create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix_logreg), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for Logistic Regression', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for Logistic Regression:", accuracy_score(y_test, predictions))

#from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test,predictions)*100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,predictions) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,predictions)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

def get_roc(y_test, y_pred_prob):
    # Compute ROC curve and ROC area for the Logistic Regression model
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)

    # Create a figure with a specific size
    plt.figure(figsize=(8, 6))  # Adjust the width and height as needed

    # Plot ROC curve
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()

# Assuming you have the 'y_test' and 'predictions' from the Logistic Regression model
get_roc(y_test, predictions)

get_prec_recall(y_test,predictions)

"""###Logistic Regression Model evaluation based on K-fold cross-validation using cross_validate() function"""

from sklearn.model_selection import cross_validate

scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

results = cross_validate(logmodel, X, y, cv=10, scoring=list(scoring.values()),
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(logmodel.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss'
                               else results['test_%s' % list(scoring.values())[sc]].mean(),
                               results['test_%s' % list(scoring.values())[sc]].std()))

"""###It can be seen that Logistic Regression has an accuracy of 85.9 %

### Naive Bayes

Running Naive Bayes first to see model performance.
"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train.toarray(), y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test.toarray())

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

# Applying k-Fold Cross Validation
accuracies = cross_val_score(estimator=classifier, X=X_train.toarray(), y=y_train, cv=10)
accuracies.mean()

from sklearn.metrics import confusion_matrix, accuracy_score
#print(confusion_matrix(y_test,predictions))
cnf_matrix_nb = confusion_matrix(y_test, y_pred)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix_nb), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for Naive Bayes', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for Naive Bayes:", accuracy_score(y_test, y_pred))

#from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test,y_pred)*100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,y_pred) #y_target, y_pred

#MAE L2 loss function - Should be close to 0
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test,y_pred) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,y_pred)

# Get ROC curve for Naive Bayes

get_roc(y_test,y_pred)

get_prec_recall(y_test,y_pred)

# Applying k-Fold Cross Validation for test set
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = logmodel, X = X_test, y = y_test, cv = 10)
accuracies.mean()

from sklearn.model_selection import cross_validate

scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

results = cross_validate(classifier, X_train.toarray(), y_train, cv=10, scoring=list(scoring.values()),
                         return_train_score=False)
print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(classifier.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss'
                               else results['test_%s' % list(scoring.values())[sc]].mean(),
                               results['test_%s' % list(scoring.values())[sc]].std()))

"""###The Naive Bayes classfier has an accuracy of 82.0%.

###DECISION TREE
"""

from sklearn.tree import DecisionTreeClassifier

decclassifier = DecisionTreeClassifier(criterion='entropy')
decclassifier.fit(X_train, y_train)

y_pred = decclassifier.predict(X_test)

decclassifier.feature_importances_

"""###VALIDATION"""

#Validation
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

# Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = decclassifier, X = X_train, y = y_train, cv = 10)
accuracies.mean()

from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt

cnf_matrix_dectree = confusion_matrix(y_test, y_pred)

# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix_dectree), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for Decision Tree', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for Decision Tree:", accuracy_score(y_test, y_pred))

#from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test,y_pred)*100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,y_pred) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,y_pred)

# Get ROC curve for Decision Tree

get_roc(y_test,y_pred)

get_prec_recall(y_test,y_pred)

# Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = decclassifier, X = X_train, y = y_train, cv = 10)
accuracies.mean()

from sklearn.model_selection import cross_validate

scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

results = cross_validate(decclassifier, X_train, y_train, cv=10, scoring=list(scoring.values()),
                         return_train_score=False)
print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(decclassifier.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss'
                               else results['test_%s' % list(scoring.values())[sc]].mean(),
                               results['test_%s' % list(scoring.values())[sc]].std()))

"""### RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=1000)
rfc.fit(X_train, y_train)

rfc.feature_importances_

"""###VALIDATION"""

from sklearn.metrics import confusion_matrix

rfc_pred = rfc.predict(X_test)  # Assuming you have trained and fitted the Random Forest Classifier (rfc) before this step

print(confusion_matrix(y_test, rfc_pred))

print(classification_report(y_test,rfc_pred))

from sklearn.metrics import accuracy_score

print("Accuracy: ", accuracy_score(y_test, rfc_pred) * 100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,rfc_pred) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,rfc_pred)

from sklearn.metrics import confusion_matrix, accuracy_score

cnf_matrix_rf = confusion_matrix(y_test, rfc_pred)

# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix_rf), annot=True, cmap="RdBu", fmt='g')
plt.tight_layout()
plt.title('Confusion matrix for Random Forest', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

accuracy_rf = accuracy_score(y_test, rfc_pred)
print("Model Accuracy for Random Forest: {:.2f}%".format(accuracy_rf * 100))

get_roc(y_test,rfc_pred)

get_prec_recall(y_test,rfc_pred)

#Applying k-Fold Cross Validation
#from sklearn.model_selection import cross_val_score
#accuracies = cross_val_score(estimator = rfc, X = X_train, y = y_train, cv = 10)
#accuracies.mean()

"""###rfc accuracy"""

#from sklearn.model_selection import cross_validate

#scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

#results = cross_validate(rfc, X_train, y_train, cv=10, scoring=list(scoring.values()),
                         #return_train_score=False)
#print('K-fold cross-validation results:')
#for sc in range(len(scoring)):
    #print(decclassifier.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               #if list(scoring.values())[sc]=='neg_log_loss'
                               #else results['test_%s' % list(scoring.values())[sc]].mean(),
                               #results['test_%s' % list(scoring.values())[sc]].std()))



"""Random forests provides 92% accuracy and higher cross validation auc than other classfication algorithms.

###SVM
"""

# Fitting Kernel SVM to the Training set
from sklearn.svm import SVC
svcclassifier = SVC(kernel = 'rbf', random_state = 0, gamma=0.8, C=100, probability=True)
svcclassifier.fit(X_train, y_train)

svc_pred = svcclassifier.predict(X_test)

#print (svcclassifier.get_feature_names())
print(classification_report(y_test,svc_pred))

#from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test,svc_pred)*100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,svc_pred) #y_target, y_pred

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,svc_pred) #y_target, y_pred

from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Making the Confusion Matrix
cm = confusion_matrix(y_test, svc_pred)
#print(cm)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for SVM', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for SVM:", accuracy_score(y_test, svc_pred) * 100)

get_roc(y_test,svc_pred)

get_prec_recall(y_test,svc_pred)

#Applying k-Fold Cross Validation
#from sklearn.model_selection import cross_val_score
#accuracies = cross_val_score(estimator = svcclassifier, X = X_train, y = y_train, cv = 10)
#accuracies.mean()

#from sklearn.model_selection import cross_validate

#scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

#results = cross_validate(svcclassifier, X_train, y_train, cv=10, scoring=list(scoring.values()), return_train_score=False)

#print('K-fold cross-validation results:')
#for sc in range(len(scoring)):
    #print(svcclassifier.__class__.__name__ + " average %s: %.3f (+/-%.3f)" % (
        #list(scoring.keys())[sc],
        #-results['test_%s' % list(scoring.values())[sc]].mean() if list(scoring.values())[sc] == 'neg_log_loss' else results[
            #'test_%s' % list(scoring.values())[sc]].mean(),
        #results['test_%s' % list(scoring.values())[sc]].std()
    #))

"""###SVC WITH GRID SEARCH"""

#from sklearn.model_selection import GridSearchCV
#from sklearn.svm import SVC

# Define the hyperparameters and their possible values
#param_grid = {
    #'C': [0.1, 1, 10, 100],      # Regularization parameter
    #'gamma': [0.001, 0.01, 0.1, 1],  # Kernel coefficient
    #'kernel': ['rbf']             # Kernel type ('rbf' for radial basis function)
#}

# Create the SVM classifier
#svcclassifier = SVC(probability=True, random_state=0)

# Create the GridSearchCV object
#grid_search = GridSearchCV(estimator=svcclassifier, param_grid=param_grid,
                           #cv=10, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the grid search to your training data
#grid_search.fit(X_train, y_train)

# Get the best hyperparameters
#best_params = grid_search.best_params_
#print("Best Hyperparameters:", best_params)

"""###XG BOOST CLASSIFIER"""

# Fitting XGBoost to the Training set
from xgboost import XGBClassifier
xgclassifier = XGBClassifier()
xgclassifier.fit(X_train, y_train)

# Predicting the Test set results
xg_pred = xgclassifier.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Making the Confusion Matrix
cm = confusion_matrix(y_test, xg_pred)
print(cm)
print("Model Accuracy for XGBoost:", accuracy_score(y_test, xg_pred))

print(classification_report(y_test,xg_pred))

from sklearn.metrics import accuracy_score

print("Accuracy:", accuracy_score(y_test, xg_pred) * 100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,xg_pred) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,xg_pred)

from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_test, xg_pred)
#print(cm)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for XGBoost', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for XGBoost:", accuracy_score(y_test, xg_pred) * 100)

get_roc(y_test,xg_pred)

get_prec_recall(y_test,xg_pred)

# Applying k-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = decclassifier, X = X_train, y = y_train, cv = 10)
accuracies.mean()

"""###KNN CLASSIFIER"""

from sklearn.decomposition import TruncatedSVD

# Replace PCA with TruncatedSVD
pca = TruncatedSVD(n_components=5)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_

from sklearn.neighbors import KNeighborsClassifier
knnclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knnclassifier.fit(X_train_pca, y_train)

# Predicting the Test set results
knn_pred = knnclassifier.predict(X_test_pca)

from sklearn.metrics import classification_report
print(classification_report(y_test,knn_pred))

#from sklearn.metrics import accuracy_score
print ("Accuracy : ", accuracy_score(y_test,knn_pred)*100)

#MAE L1 loss function - Should be close to 0
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test,knn_pred) #y_target, y_pred

# Log Loss  - Should be close to 0 - Only for classification models
from sklearn.metrics import log_loss
log_loss(y_test,knn_pred)

from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, knn_pred)
#print(cm)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="RdBu", fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for KNN', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
print("Model Accuracy for KNN:", accuracy_score(y_test, knn_pred) * 100)

get_roc(y_test,knn_pred)

get_prec_recall(y_test,knn_pred)

from sklearn.model_selection import cross_validate

scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

results = cross_validate(knnclassifier, X_train, y_train, cv=10, scoring=list(scoring.values()),
                         return_train_score=False)
print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(knnclassifier.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss'
                               else results['test_%s' % list(scoring.values())[sc]].mean(),
                               results['test_%s' % list(scoring.values())[sc]].std()))

from sklearn.ensemble import VotingClassifier

# Create a list of (name, model) tuples
estimators = [
    ('logistic', logmodel),
    ('naive_bayes', classifier),
    ('decision_tree', decclassifier),
    ('random_forest', rfc),
    ('svc', svcclassifier),
    ('xgboost', xgclassifier),
    ('knn', knnclassifier)
]

# Convert sparse matrices to dense arrays
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()

# Create a VotingClassifier with majority voting
voting_classifier = VotingClassifier(estimators, voting='hard')

# Fit the VotingClassifier on the training data
voting_classifier.fit(X_train_dense, y_train)

# Predict using the VotingClassifier
voting_predictions = voting_classifier.predict(X_test_dense)